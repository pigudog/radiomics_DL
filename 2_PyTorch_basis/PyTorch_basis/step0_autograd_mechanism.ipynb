{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The most impressive thing that the framework does is that it calculates the entire backward propagation for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For differentiation, we can manually define it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4331,  0.3370,  1.5373, -0.3957],\n",
       "        [-0.7298,  0.9322, -0.0695,  1.0893],\n",
       "        [ 2.6122, -0.8767,  0.9718,  1.1008]], requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method 1\n",
    "x = torch.randn(3,4,requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6246, -3.0934, -0.6821,  0.0772],\n",
       "        [-0.0456,  1.2092, -0.1928,  0.3894],\n",
       "        [ 2.0155, -1.1630, -0.6860,  1.1175]], requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Method 2\n",
    "x = torch.randn(3,4)\n",
    "x.requires_grad=True\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = torch.randn(3,4,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.6527, -4.3626, -2.0832, -0.9918],\n",
       "        [-0.0331,  1.7364,  0.6924,  0.1056],\n",
       "        [ 2.9148,  0.5487, -0.2787,  3.5601]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = x + b\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.4613, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = t.sum()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although 'requires_grad' is not specified for 't', it will be used by default if needed if you use `backward`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.requires_grad, b.requires_grad, t.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 举个例子看一下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./img/2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the flow of calculation\n",
    "x = torch.rand(1)\n",
    "b = torch.rand(1, requires_grad = True)\n",
    "w = torch.rand(1, requires_grad = True)\n",
    "y = w * x \n",
    "z = y + b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True, True, True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.requires_grad, b.requires_grad, w.requires_grad, y.requires_grad # please noted y is also need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True, False, False)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# leaf vs root:\n",
    "x.is_leaf, w.is_leaf, b.is_leaf, y.is_leaf, z.is_leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation calculation\n",
    "\n",
    "Backpropagation is a key algorithm in training neural networks, which uses a gradient descent method to minimize the cost function. It calculates the gradient of the error with respect to the network's weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.backward(retain_graph=True) # If the gradients are not cleared, they will accumulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of training a machine learning model, it's crucial to zero out the gradients at the beginning of each optimization step, because by default, gradients are accumulated in buffers (i.e., not overwritten) whenever a forward and backward propagation is performed. \n",
    "\n",
    "Otherwise, this might lead to incorrect model training results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.3093])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.backward(retain_graph=True) # If the gradients are not cleared, they will accumulate\n",
    "b.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Let's try a linear regression for experiment\n",
    "\n",
    "Linear regression is a basic and commonly used type of predictive analysis, especially useful in establishing a relationship between two or more variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Construct a set of input data 'X' and its corresponding labels 'y'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `numpy.reshape` function provides a way to reorganize an array according to specified dimensions without changing its data. \n",
    "\n",
    "Here, the reshape(-1, 1) function aims to shape the x_train array to have one column and as many rows as needed to accommodate the original data.\n",
    "\n",
    "In the context of numpy's reshape function, -1 is often used as an argument. This value is used to automatically infer the length of a certain dimension, based on other dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.]\n",
      " [ 1.]\n",
      " [ 2.]\n",
      " [ 3.]\n",
      " [ 4.]\n",
      " [ 5.]\n",
      " [ 6.]\n",
      " [ 7.]\n",
      " [ 8.]\n",
      " [ 9.]\n",
      " [10.]] \n",
      "\n",
      "shape (11, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x_values = [i for i in range(11)]\n",
    "x_train = np.array(x_values, dtype=np.float32)\n",
    "x_train = x_train.reshape(-1, 1)\n",
    "print(x_train,'\\n')\n",
    "print('shape',x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.]\n",
      " [ 3.]\n",
      " [ 5.]\n",
      " [ 7.]\n",
      " [ 9.]\n",
      " [11.]\n",
      " [13.]\n",
      " [15.]\n",
      " [17.]\n",
      " [19.]\n",
      " [21.]] \n",
      "\n",
      "shape (11, 1)\n"
     ]
    }
   ],
   "source": [
    "y_values = [2*i + 1 for i in x_values]\n",
    "y_train = np.array(y_values, dtype=np.float32)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "print(y_train,'\\n')\n",
    "print('shape',y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression Model\n",
    "\n",
    "     In fact, linear regression is just a fully connected layer without an activation function.\n",
    "\n",
    "A fully connected layer in a neural network is a layer where each input node is connected to each output node. In the context of a linear regression model, this would mean that all input feature variables contribute linearly to the output predictions. As mentioned, we do not apply an activation function in linear regression, as the output is a linear combination of input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = 1\n",
    "output_dim = 1\n",
    "\n",
    "model = LinearRegressionModel(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegressionModel(\n",
       "  (linear): Linear(in_features=1, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the parameters and loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 1000\n",
    "learning_rate = 0.01\n",
    "# Stochastic Gradient Descent (SGD) is used for optimization, with the aforementioned learning rate.\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# calculates the Mean Squared Error (MSE) between each element in the input x and target y\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, loss 0.10558705031871796\n",
      "epoch 100, loss 0.06022291257977486\n",
      "epoch 150, loss 0.034348901361227036\n",
      "epoch 200, loss 0.01959127001464367\n",
      "epoch 250, loss 0.011174176819622517\n",
      "epoch 300, loss 0.006373362150043249\n",
      "epoch 350, loss 0.003635118715465069\n",
      "epoch 400, loss 0.002073340117931366\n",
      "epoch 450, loss 0.0011825737310573459\n",
      "epoch 500, loss 0.0006744735292159021\n",
      "epoch 550, loss 0.00038468989077955484\n",
      "epoch 600, loss 0.00021941652812529355\n",
      "epoch 650, loss 0.00012514815898612142\n",
      "epoch 700, loss 7.137949432944879e-05\n",
      "epoch 750, loss 4.071067814948037e-05\n",
      "epoch 800, loss 2.3219095965032466e-05\n",
      "epoch 850, loss 1.324387631029822e-05\n",
      "epoch 900, loss 7.553593150078086e-06\n",
      "epoch 950, loss 4.3082504816993605e-06\n",
      "epoch 1000, loss 2.457582922943402e-06\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    epoch += 1\n",
    "    # Remember to convert ndarray into tensor\n",
    "    inputs = torch.from_numpy(x_train)\n",
    "    labels = torch.from_numpy(y_train)\n",
    "\n",
    "    # At each iteration, the gradient must be cleared to zero\n",
    "    optimizer.zero_grad() \n",
    "\n",
    "    # Forward propagation\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # Calculate the loss\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    # Back propagation\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weight parameters\n",
    "    optimizer.step()\n",
    "    if epoch % 50 == 0:\n",
    "        print('epoch {}, loss {}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model prediction results were tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.99708414],\n",
       "       [ 2.997504  ],\n",
       "       [ 4.997924  ],\n",
       "       [ 6.998344  ],\n",
       "       [ 8.998764  ],\n",
       "       [10.999184  ],\n",
       "       [12.999603  ],\n",
       "       [15.000023  ],\n",
       "       [17.000443  ],\n",
       "       [19.000862  ],\n",
       "       [21.001282  ]], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = model(torch.from_numpy(x_train).requires_grad_()).data.numpy()\n",
    "predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model saving and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training using GPU\n",
    "\n",
    "    You just need to load the data and model into CUDA.\n",
    "\n",
    "In the context of machine learning, especially deep learning, training models on a GPU can significantly decrease training times because GPUs are designed to efficiently perform matrix operations, which are fundamental in deep learning. By specifying to use CUDA (an extension of the programming language developed by NVIDIA for general computing on its GPUs), you direct your model and data to be loaded onto the GPU, if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 50, loss 0.2915838360786438\n",
      "epoch 100, loss 0.16630853712558746\n",
      "epoch 150, loss 0.09485612064599991\n",
      "epoch 200, loss 0.05410239100456238\n",
      "epoch 250, loss 0.03085799515247345\n",
      "epoch 300, loss 0.017600249499082565\n",
      "epoch 350, loss 0.010038497857749462\n",
      "epoch 400, loss 0.005725600756704807\n",
      "epoch 450, loss 0.003265665378421545\n",
      "epoch 500, loss 0.0018626069650053978\n",
      "epoch 550, loss 0.0010623658308759332\n",
      "epoch 600, loss 0.0006059351726435125\n",
      "epoch 650, loss 0.00034560466883704066\n",
      "epoch 700, loss 0.00019711993809323758\n",
      "epoch 750, loss 0.00011243174958508462\n",
      "epoch 800, loss 6.412637594621629e-05\n",
      "epoch 850, loss 3.657592969830148e-05\n",
      "epoch 900, loss 2.0861582015641034e-05\n",
      "epoch 950, loss 1.1898667253262829e-05\n",
      "epoch 1000, loss 6.78708011037088e-06\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "input_dim = 1\n",
    "output_dim = 1\n",
    "\n",
    "model = LinearRegressionModel(input_dim, output_dim)\n",
    "\n",
    "# gpu acceleration\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    epoch += 1\n",
    "    inputs = torch.from_numpy(x_train).to(device)\n",
    "    labels = torch.from_numpy(y_train).to(device)\n",
    "\n",
    "    optimizer.zero_grad() \n",
    "\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        print('epoch {}, loss {}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
